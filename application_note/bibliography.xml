<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<b:Sources xmlns:b="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" xmlns="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" SelectedStyle="">
    <b:Source>
        <b:Year>2020</b:Year>
        <b:Volume>7</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>A Comparative Study of Widely Used Image Detection Algorithms</b:Title>
        <b:Tag>Zota2020</b:Tag>
        <b:URL>https://www.irjet.net/archives/V7/i7/IRJET-V7I7903.pdf</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Zota</b:Last>
                        <b:First>Harsh</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Dhande</b:Last>
                        <b:First>Manoj</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>5183–5187</b:Pages>
        <b:Month>July</b:Month>
        <b:JournalName>International Research Journal of Engineering and Technology</b:JournalName>
        <b:Number>7</b:Number>
        <b:StandardNumber> ISSN: 2395-0056</b:StandardNumber>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Detection evaluation</b:Title>
        <b:Tag>COCOdet</b:Tag>
        <b:URL>https://cocodataset.org/#detection-eval [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>COCO</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Detection evaluation</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Co-DETR</b:Title>
        <b:Tag>Zong</b:Tag>
        <b:URL>https://github.com/Sense-X/Co-DETR [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Zong</b:Last>
                        <b:First>Zhuofan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Song</b:Last>
                        <b:First>Guanglu</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Liu</b:Last>
                        <b:First>Yu</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Co-DETR</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2019</b:Year>
        <b:Volume>PP</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Object Detection With Deep Learning: A Review</b:Title>
        <b:Tag>Zhao2019</b:Tag>
        <b:DOI>10.1109/TNNLS.2018.2876865</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Zhao</b:Last>
                        <b:First>Zhong-Qiu</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Zheng</b:Last>
                        <b:First>Peng</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Xu</b:Last>
                        <b:First>Shou-Tao</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Wu</b:Last>
                        <b:First>Xindong</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>1-21</b:Pages>
        <b:Month>January</b:Month>
        <b:JournalName>IEEE Transactions on Neural Networks and Learning Systems</b:JournalName>
    </b:Source>
    <b:Source>
        <b:Year>2023</b:Year>
        <b:Volume>1</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Comparative Study of Some Deep Learning Object Detection Algorithms: R-CNN, FAST RCNN, FASTER R-CNN, SSD, and YOLO</b:Title>
        <b:Tag>Olorunshola2023</b:Tag>
        <b:URL>https://www.ejmanager.com/mnstemps/262/262-1681947210.pdf?t=1710336581</b:URL>
        <b:DOI>10.5455/njeas.150264</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Olorunshola</b:Last>
                        <b:Middle>Ezekiel</b:Middle>
                        <b:First>Oluwaseyi</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Ademuwagun</b:Last>
                        <b:Middle>Kolade</b:Middle>
                        <b:First>Adeniran</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Dyaji</b:Last>
                        <b:First>Charles</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>70–80</b:Pages>
        <b:Month>September</b:Month>
        <b:JournalName>Nile Journal of Engineering &amp; Applied Science</b:JournalName>
        <b:Number>1</b:Number>
    </b:Source>
    <b:Source>
        <b:Year>2024</b:Year>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>YOLOv9: A Leap Forward in Object Detection Technology</b:Title>
        <b:Tag>Jocher2024</b:Tag>
        <b:URL>https://docs.ultralytics.com/models/yolov9/ [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Jocher</b:Last>
                        <b:First>Glenn</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Qaddoumi</b:Last>
                        <b:First>Burhan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Laughing-q</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>February</b:Month>
        <b:Day>26</b:Day>
        <b:InternetSiteTitle>YOLOv9: A Leap Forward in Object Detection Technology</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Models and pre-trained weights: Object detection</b:Title>
        <b:Tag>PyTorch</b:Tag>
        <b:URL>https://pytorch.org/vision/stable/models.html#object-detection [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PyTorch</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Models and pre-trained weights: Object detection</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2021</b:Year>
        <b:Volume>12</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Object Detection Approaches in Images: A Weighted Scoring Model based Comparative Study</b:Title>
        <b:Publisher>The Science and Information Organization</b:Publisher>
        <b:Tag>Ouchra2021</b:Tag>
        <b:URL>http://dx.doi.org/10.14569/IJACSA.2021.0120831</b:URL>
        <b:DOI>10.14569/IJACSA.2021.0120831</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Ouchra</b:Last>
                        <b:First>Hafsa</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Belangour</b:Last>
                        <b:First>Abdessamad</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:JournalName>International Journal of Advanced Computer Science and Applications</b:JournalName>
        <b:Number>8</b:Number>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Calculate Computational Efficiency of Deep Learning Models with FLOPs and MACs</b:Title>
        <b:Tag>Li</b:Tag>
        <b:URL>https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Li</b:Last>
                        <b:First>Danny</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Calculate Computational Efficiency of Deep Learning Models with FLOPs and MACs</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Object Detection on COCO test-dev</b:Title>
        <b:Tag>PapersWithCode</b:Tag>
        <b:URL>https://paperswithcode.com/sota/object-detection-on-coco [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PapersWithCode</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Object Detection on COCO test-dev</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>InternImage</b:Title>
        <b:Tag>Wanga</b:Tag>
        <b:URL>https://github.com/opengvlab/internimage [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Wang</b:Last>
                        <b:First>Wenhai</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Dai</b:Last>
                        <b:First>Jifeng</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Chen</b:Last>
                        <b:First>Zhe</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Huang</b:Last>
                        <b:First>Zhenhang</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Li</b:Last>
                        <b:First>Zhiqi</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Zhu</b:Last>
                        <b:First>Xizhou</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Hu</b:Last>
                        <b:First>Xiaowei</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Lu</b:Last>
                        <b:First>Tong</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Lu</b:Last>
                        <b:First>Lewei</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Li</b:Last>
                        <b:First>Hongsheng</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>others</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>InternImage</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>R-FCN</b:Title>
        <b:Tag>Dai</b:Tag>
        <b:URL>https://github.com/daijifeng001/R-FCN [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Dai</b:Last>
                        <b:First>Jifeng</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Li</b:Last>
                        <b:First>Yi</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Xiong</b:Last>
                        <b:First>Yuwen</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Qi</b:Last>
                        <b:First>Haozhi</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>R-FCN</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2014</b:Year>
        <b:BIBTEX_Entry>inproceedings</b:BIBTEX_Entry>
        <b:SourceType>ConferenceProceedings</b:SourceType>
        <b:Title>Microsoft COCO: Common Objects in Context</b:Title>
        <b:Publisher>Springer International Publishing</b:Publisher>
        <b:BIBTEX_Abstract>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</b:BIBTEX_Abstract>
        <b:Tag>Lin2014</b:Tag>
        <b:BookTitle>Computer Vision – ECCV 2014</b:BookTitle>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Lin</b:Last>
                        <b:First>Tsung-Yi</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Maire</b:Last>
                        <b:First>Michael</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Belongie</b:Last>
                        <b:First>Serge</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Hays</b:Last>
                        <b:First>James</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Perona</b:Last>
                        <b:First>Pietro</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Ramanan</b:Last>
                        <b:First>Deva</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Dollár</b:Last>
                        <b:First>Piotr</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Zitnick</b:Last>
                        <b:Middle>Lawrence</b:Middle>
                        <b:First>C.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
            <b:Editor>
                <b:NameList>
                    <b:Person>
                        <b:Last>Fleet</b:Last>
                        <b:First>David</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Pajdla</b:Last>
                        <b:First>Tomas</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Schiele</b:Last>
                        <b:First>Bernt</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Tuytelaars</b:Last>
                        <b:First>Tinne</b:First>
                    </b:Person>
                </b:NameList>
            </b:Editor>
        </b:Author>
        <b:Pages>740–755</b:Pages>
        <b:StandardNumber> ISBN: 978-3-319-10602-1</b:StandardNumber>
        <b:ConferenceName>Computer Vision – ECCV 2014</b:ConferenceName>
        <b:City>Cham</b:City>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>YOLOX</b:Title>
        <b:Tag>Ge</b:Tag>
        <b:URL>https://github.com/Megvii-BaseDetection/YOLOX [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Ge</b:Last>
                        <b:First>Zheng</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Liu</b:Last>
                        <b:First>Songtao</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Wang</b:Last>
                        <b:First>Feng</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Li</b:Last>
                        <b:First>Zeming</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Sun</b:Last>
                        <b:First>Jian</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>YOLOX</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>FCOS</b:Title>
        <b:Tag>PyTorchc</b:Tag>
        <b:URL>https://pytorch.org/vision/main/models/fcos.html [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PyTorch</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>FCOS</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Faster R-CNN</b:Title>
        <b:Tag>PyTorcha</b:Tag>
        <b:URL>https://pytorch.org/vision/main/models/faster_rcnn.html [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PyTorch</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Faster R-CNN</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>SSD</b:Title>
        <b:Tag>PyTorchb</b:Tag>
        <b:URL>https://pytorch.org/vision/main/models/ssd.html [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PyTorch</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>SSD</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Models</b:Title>
        <b:Tag>Kaggle</b:Tag>
        <b:URL>https://www.kaggle.com/models?tfhub-redirect=true&amp;task=17074&amp;fine-tunable=true [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Kaggle</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Models</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Torchvision</b:Title>
        <b:Tag>PapersWithCodea</b:Tag>
        <b:URL>https://paperswithcode.com/lib/torchvision [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PapersWithCode</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Torchvision</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>yolov7</b:Title>
        <b:Tag>Wang</b:Tag>
        <b:URL>https://github.com/WongKinYiu/yolov7 [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Wang</b:Last>
                        <b:First>Chien-Yao</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Bochkovskiy</b:Last>
                        <b:First>Alexey</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Liao</b:Last>
                        <b:Middle>Mark</b:Middle>
                        <b:First>Hong-Yuan</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>yolov7</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2023</b:Year>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Models Supported by Ultralytics</b:Title>
        <b:Tag>Jocher2023</b:Tag>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Jocher</b:Last>
                        <b:First>Glenn</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Laughing-q</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>November</b:Month>
        <b:Day>12</b:Day>
        <b:InternetSiteTitle>Models Supported by Ultralytics</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2021</b:Year>
        <b:Volume>8</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Comparative analysis of deep learning image detection algorithms</b:Title>
        <b:BIBTEX_Abstract>A computer views all kinds of visual media as an array of numerical values. As a consequence of this approach, they require image processing algorithms to inspect contents of images. This project compares 3 major image processing algorithms: Single Shot Detection (SSD), Faster Region based Convolutional Neural Networks (Faster R-CNN), and You Only Look Once (YOLO) to find the fastest and most efficient of three. In this comparative analysis, using the Microsoft COCO (Common Object in Context) dataset, the performance of these three algorithms is evaluated and their strengths and limitations are analysed based on parameters such as accuracy, precision and F1 score. From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms.</b:BIBTEX_Abstract>
        <b:Tag>Srivastava2021</b:Tag>
        <b:URL>https://doi.org/10.1186/s40537-021-00434-w</b:URL>
        <b:DOI>10.1186/s40537-021-00434-w</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Srivastava</b:Last>
                        <b:First>Shrey</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Divekar</b:Last>
                        <b:Middle>Vishvas</b:Middle>
                        <b:First>Amit</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Anilkumar</b:Last>
                        <b:First>Chandu</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Naik</b:Last>
                        <b:First>Ishika</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Kulkarni</b:Last>
                        <b:First>Ved</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Pattabiraman</b:Last>
                        <b:First>V.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>66</b:Pages>
        <b:JournalName>Journal of Big Data</b:JournalName>
        <b:Number>1</b:Number>
        <b:StandardNumber> ISSN: 2196-1115</b:StandardNumber>
    </b:Source>
    <b:Source>
        <b:Year>2023</b:Year>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Baidu's RT-DETR: A Vision Transformer-Based Real-Time Object Detector</b:Title>
        <b:Tag>Jocher2023b</b:Tag>
        <b:URL>https://docs.ultralytics.com/models/rtdetr/ [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Jocher</b:Last>
                        <b:First>Glenn</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>November</b:Month>
        <b:Day>12</b:Day>
        <b:InternetSiteTitle>Baidu's RT-DETR: A Vision Transformer-Based Real-Time Object Detector</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2019</b:Year>
        <b:BIBTEX_Entry>misc</b:BIBTEX_Entry>
        <b:SourceType>Misc</b:SourceType>
        <b:Title>Detectron2</b:Title>
        <b:BIBTEX_HowPublished>\url{https://github.com/facebookresearch/detectron2}</b:BIBTEX_HowPublished>
        <b:Tag>Wu2019</b:Tag>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Wu</b:Last>
                        <b:First>Yuxin</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Kirillov</b:Last>
                        <b:First>Alexander</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Massa</b:Last>
                        <b:First>Francisco</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Lo</b:Last>
                        <b:First>Wan-Yen</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Girshick</b:Last>
                        <b:First>Ross</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:PublicationTitle>Detectron2</b:PublicationTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>RetinaNet</b:Title>
        <b:Tag>PyTorchd</b:Tag>
        <b:URL>https://pytorch.org/vision/main/models/retinanet.html [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PyTorch</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>RetinaNet</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2016</b:Year>
        <b:BIBTEX_Entry>inproceedings</b:BIBTEX_Entry>
        <b:SourceType>ConferenceProceedings</b:SourceType>
        <b:Title>SSD: Single Shot MultiBox Detector</b:Title>
        <b:Publisher>Springer International Publishing</b:Publisher>
        <b:BIBTEX_Abstract>We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For $$300 \backslashtimes 300$$300\texttimes300input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for $$512 \backslashtimes 512$$512\texttimes512input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.</b:BIBTEX_Abstract>
        <b:Tag>Liu2016</b:Tag>
        <b:BookTitle>Computer Vision – ECCV 2016</b:BookTitle>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Liu</b:Last>
                        <b:First>Wei</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Anguelov</b:Last>
                        <b:First>Dragomir</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Erhan</b:Last>
                        <b:First>Dumitru</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Szegedy</b:Last>
                        <b:First>Christian</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Reed</b:Last>
                        <b:First>Scott</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Fu</b:Last>
                        <b:First>Cheng-Yang</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Berg</b:Last>
                        <b:Middle>C.</b:Middle>
                        <b:First>Alexander</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
            <b:Editor>
                <b:NameList>
                    <b:Person>
                        <b:Last>Leibe</b:Last>
                        <b:First>Bastian</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Matas</b:Last>
                        <b:First>Jiri</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Sebe</b:Last>
                        <b:First>Nicu</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Welling</b:Last>
                        <b:First>Max</b:First>
                    </b:Person>
                </b:NameList>
            </b:Editor>
        </b:Author>
        <b:Pages>21–37</b:Pages>
        <b:StandardNumber> ISBN: 978-3-319-46448-0</b:StandardNumber>
        <b:ConferenceName>Computer Vision – ECCV 2016</b:ConferenceName>
        <b:City>Cham</b:City>
    </b:Source>
    <b:Source>
        <b:Year>2020</b:Year>
        <b:Volume>844</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>A review: Comparison of performance metrics of pretrained models for object detection using the TensorFlow framework</b:Title>
        <b:Publisher>IOP Publishing</b:Publisher>
        <b:BIBTEX_Abstract>Advances in parallel computing, GPU technology and deep learning facilitate the tools for processing complex images. The purpose of this research was focused on a review of the state of the art, related to the performance of pre-trained models for the detection of objects in order to make a comparison of these algorithms in terms of reliability, ac-curacy, time processed and Problems detected The consulted models are based on the Python programming language, the use of libraries based on TensorFlow, OpenCv and free image databases (Microsoft COCO and PAS-CAL VOC 2007/2012). These systems are not only focused on the recognition and classification of the objects in the images, but also on the location of the objects within it, drawing a bounding box around the appropriate way. For this research, different pre-trained models were re-viewed for the detection of objects such as R-CNN, R-FCN, SSD (single-shot multibox) and YOLO (You Only Look Once), with different extractors of characteristics such as VGG16, ResNet, Inception, MobileNet. As a result, it is not prudent to make direct and parallel analyzes between the different architecture and models, because each case has a particular solution for each problem, the purpose of this research is to generate an approximate notion of the experiments that have been carried out and conceive a starting point in the use that they are intended to give.</b:BIBTEX_Abstract>
        <b:Tag>Sanchez2020</b:Tag>
        <b:URL>https://dx.doi.org/10.1088/1757-899X/844/1/012024</b:URL>
        <b:DOI>10.1088/1757-899X/844/1/012024</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Sanchez</b:Last>
                        <b:Middle>A.</b:Middle>
                        <b:First>S.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Romero</b:Last>
                        <b:Middle>J.</b:Middle>
                        <b:First>H.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Morales</b:Last>
                        <b:Middle>D.</b:Middle>
                        <b:First>A.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>012024</b:Pages>
        <b:Month>May</b:Month>
        <b:JournalName>IOP Conference Series: Materials Science and Engineering</b:JournalName>
        <b:Number>1</b:Number>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>Intersection over Union (IoU): Definition, Calculation, Code</b:Title>
        <b:Tag>Shah</b:Tag>
        <b:URL>https://www.v7labs.com/blog/intersection-over-union-guide [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Shah</b:Last>
                        <b:First>Deval</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>Intersection over Union (IoU): Definition, Calculation, Code</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:Year>2023</b:Year>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>YOLOv8</b:Title>
        <b:Tag>Jocher2023a</b:Tag>
        <b:URL>https://docs.ultralytics.com/models/yolov8/ [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Jocher</b:Last>
                        <b:First>Glenn</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Chaurasia</b:Last>
                        <b:First>Ayush</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Akyon</b:Last>
                        <b:Middle>C.</b:Middle>
                        <b:First>Fatih</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Laughing-q</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>November</b:Month>
        <b:Day>12</b:Day>
        <b:InternetSiteTitle>YOLOv8</b:InternetSiteTitle>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>online</b:BIBTEX_Entry>
        <b:SourceType>InternetSite</b:SourceType>
        <b:Title>NAS-FCOS: Fast Neural Architecture Search for Object Detection</b:Title>
        <b:Tag>PapersWithCodeb</b:Tag>
        <b:URL>https://paperswithcode.com/paper/nas-fcos-fast-neural-architecture-search-for [Accessed: 13/03/2024]</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>PapersWithCode</b:Last>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:InternetSiteTitle>NAS-FCOS: Fast Neural Architecture Search for Object Detection</b:InternetSiteTitle>
    </b:Source>
</b:Sources>
