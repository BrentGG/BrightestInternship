@InProceedings{Lin2014,
  author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle = {Computer Vision -- ECCV 2014},
  title     = {Microsoft COCO: Common Objects in Context},
  year      = {2014},
  address   = {Cham},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {740--755},
  publisher = {Springer International Publishing},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn      = {978-3-319-10602-1},
}

@Article{Srivastava2021,
  author   = {Srivastava, Shrey and Divekar, Amit Vishvas and Anilkumar, Chandu and Naik, Ishika and Kulkarni, Ved and Pattabiraman, V.},
  journal  = {Journal of Big Data},
  title    = {Comparative analysis of deep learning image detection algorithms},
  year     = {2021},
  issn     = {2196-1115},
  number   = {1},
  pages    = {66},
  volume   = {8},
  abstract = {A computer views all kinds of visual media as an array of numerical values. As a consequence of this approach, they require image processing algorithms to inspect contents of images. This project compares 3 major image processing algorithms: Single Shot Detection (SSD), Faster Region based Convolutional Neural Networks (Faster R-CNN), and You Only Look Once (YOLO) to find the fastest and most efficient of three. In this comparative analysis, using the Microsoft COCO (Common Object in Context) dataset, the performance of these three algorithms is evaluated and their strengths and limitations are analysed based on parameters such as accuracy, precision and F1 score. From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms.},
  doi      = {10.1186/s40537-021-00434-w},
  refid    = {Srivastava2021},
  url      = {https://doi.org/10.1186/s40537-021-00434-w},
}

@Article{Zota2020,
  author  = {Harsh Zota and Manoj Dhande},
  journal = {International Research Journal of Engineering and Technology},
  title   = {A Comparative Study of Widely Used Image Detection Algorithms},
  year    = {2020},
  issn    = {2395-0056},
  month   = jul,
  number  = {7},
  pages   = {5183--5187},
  volume  = {7},
  url     = {https://www.irjet.net/archives/V7/i7/IRJET-V7I7903.pdf},
}

@Article{Olorunshola2023,
  author  = {Oluwaseyi Ezekiel Olorunshola and Adeniran Kolade Ademuwagun and Charles Dyaji},
  journal = {Nile Journal of Engineering & Applied Science},
  title   = {Comparative Study of Some Deep Learning Object Detection Algorithms: R-CNN, FAST RCNN, FASTER R-CNN, SSD, and YOLO},
  year    = {2023},
  month   = sep,
  number  = {1},
  pages   = {70--80},
  volume  = {1},
  doi     = {10.5455/njeas.150264},
  url     = {https://www.ejmanager.com/mnstemps/262/262-1681947210.pdf?t=1710336581},
}

@Article{Sanchez2020,
  author    = {S A Sanchez and H J Romero and A D Morales},
  journal   = {IOP Conference Series: Materials Science and Engineering},
  title     = {A review: Comparison of performance metrics of pretrained models for object detection using the TensorFlow framework},
  year      = {2020},
  month     = {may},
  number    = {1},
  pages     = {012024},
  volume    = {844},
  abstract  = {Advances in parallel computing, GPU technology and deep learning facilitate the tools for processing complex images. The purpose of this research was focused on a review of the state of the art, related to the performance of pre-trained models for the detection of objects in order to make a comparison of these algorithms in terms of reliability, ac-curacy, time processed and Problems detected The consulted models are based on the Python programming language, the use of libraries based on TensorFlow, OpenCv and free image databases (Microsoft COCO and PAS-CAL VOC 2007/2012). These systems are not only focused on the recognition and classification of the objects in the images, but also on the location of the objects within it, drawing a bounding box around the appropriate way. For this research, different pre-trained models were re-viewed for the detection of objects such as R-CNN, R-FCN, SSD (single-shot multibox) and YOLO (You Only Look Once), with different extractors of characteristics such as VGG16, ResNet, Inception, MobileNet. As a result, it is not prudent to make direct and parallel analyzes between the different architecture and models, because each case has a particular solution for each problem, the purpose of this research is to generate an approximate notion of the experiments that have been carried out and conceive a starting point in the use that they are intended to give.},
  doi       = {10.1088/1757-899X/844/1/012024},
  publisher = {IOP Publishing},
  url       = {https://dx.doi.org/10.1088/1757-899X/844/1/012024},
}

@Article{Zhao2019,
  author  = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  title   = {Object Detection With Deep Learning: A Review},
  year    = {2019},
  month   = {01},
  pages   = {1-21},
  volume  = {PP},
  doi     = {10.1109/TNNLS.2018.2876865},
}

@Article{Ouchra2021,
  author    = {Hafsa Ouchra and Abdessamad Belangour},
  journal   = {International Journal of Advanced Computer Science and Applications},
  title     = {Object Detection Approaches in Images: A Weighted Scoring Model based Comparative Study},
  year      = {2021},
  number    = {8},
  volume    = {12},
  doi       = {10.14569/IJACSA.2021.0120831},
  publisher = {The Science and Information Organization},
  url       = {http://dx.doi.org/10.14569/IJACSA.2021.0120831},
}

@Misc{Wu2019,
  author       = {Yuxin Wu and Alexander Kirillov and Francisco Massa and Wan-Yen Lo and Ross Girshick},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  title        = {Detectron2},
  year         = {2019},
}

@InProceedings{Liu2016,
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  booktitle = {Computer Vision -- ECCV 2016},
  title     = {SSD: Single Shot MultiBox Detector},
  year      = {2016},
  address   = {Cham},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages     = {21--37},
  publisher = {Springer International Publishing},
  abstract  = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$}300{\texttimes}300input, SSD achieves 74.3 {\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$}512{\texttimes}512input, SSD achieves 76.9 {\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  isbn      = {978-3-319-46448-0},
}

@Online{COCOdet,
  author = {COCO},
  title  = {Detection evaluation},
  url    = {{https://cocodataset.org/#detection-eval } [Accessed: 13/03/2024]},
}

@Online{Shah,
  author = {Deval Shah},
  title  = {Intersection over Union (IoU): Definition, Calculation, Code},
  url    = {{https://www.v7labs.com/blog/intersection-over-union-guide} [Accessed: 13/03/2024]},
}

@Online{Li,
  author = {Danny Li},
  title  = {Calculate Computational Efficiency of Deep Learning Models with FLOPs and MACs},
  url    = {{https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html} [Accessed: 13/03/2024]},
}

@Online{PyTorch,
  author = {PyTorch},
  title  = {Models and pre-trained weights: Object detection},
  url    = {{https://pytorch.org/vision/stable/models.html#object-detection} [Accessed: 13/03/2024]},
}

@Online{Kaggle,
  author = {Kaggle},
  title  = {Models},
  url    = {{https://www.kaggle.com/models?tfhub-redirect=true&task=17074&fine-tunable=true} [Accessed: 13/03/2024]},
}

@Online{Jocher2023,
  author = {Glenn Jocher and Laughing-q},
  day    = {12},
  month  = {11},
  title  = {Models Supported by Ultralytics},
  utl    = {{https://docs.ultralytics.com/models/#contributing-new-models} [Accessed: 13/03/2024]},
  year   = {2023},
}

@Online{PapersWithCode,
  author = {PapersWithCode},
  title  = {Object Detection on COCO test-dev},
  url    = {{https://paperswithcode.com/sota/object-detection-on-coco} [Accessed: 13/03/2024]},
}

@Online{Ge,
  author = {Zheng Ge and Songtao Liu and Feng Wang and Zeming Li and Jian Sun},
  title  = {YOLOX},
  url    = {{https://github.com/Megvii-BaseDetection/YOLOX} [Accessed: 13/03/2024]},
}

@Online{Wang,
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  title  = {yolov7},
  url    = {{https://github.com/WongKinYiu/yolov7} [Accessed: 13/03/2024]},
}

@Online{Jocher2023a,
  author = {Glenn Jocher and Ayush Chaurasia and Fatih C. Akyon and Laughing-q},
  day    = {12},
  month  = {11},
  title  = {YOLOv8},
  url    = {{https://docs.ultralytics.com/models/yolov8/} [Accessed: 13/03/2024]},
  year   = {2023},
}

@Online{Jocher2024,
  author = {Glenn Jocher and Burhan Qaddoumi and Laughing-q},
  day    = {26},
  month  = {02},
  title  = {YOLOv9: A Leap Forward in Object Detection Technology},
  url    = {{https://docs.ultralytics.com/models/yolov9/} [Accessed: 13/03/2024]},
  year   = {2024},
}

@Online{Jocher2023b,
  author = {Glenn Jocher},
  day    = {12},
  month  = {11},
  title  = {Baidu's RT-DETR: A Vision Transformer-Based Real-Time Object Detector},
  url    = {{https://docs.ultralytics.com/models/rtdetr/} [Accessed: 13/03/2024]},
  year   = {2023},
}

@Online{Zong,
  author = {Zhuofan Zong and Guanglu Song and Yu Liu},
  title  = {Co-DETR},
  url    = {{https://github.com/Sense-X/Co-DETR} [Accessed: 13/03/2024]},
}

@Online{PyTorcha,
  author = {PyTorch},
  title  = {Faster R-CNN},
  url    = {{https://pytorch.org/vision/main/models/faster_rcnn.html} [Accessed: 13/03/2024]},
}

@Online{Dai,
  author = {Jifeng Dai and Yi Li and Yuwen Xiong and Haozhi Qi},
  title  = {R-FCN},
  url    = {{https://github.com/daijifeng001/R-FCN } [Accessed: 13/03/2024]},
}

@Online{PyTorchb,
  author = {PyTorch},
  title  = {SSD},
  url    = {{https://pytorch.org/vision/main/models/ssd.html } [Accessed: 13/03/2024]},
}

@Online{PyTorchc,
  author = {PyTorch},
  title  = {FCOS},
  url    = {{https://pytorch.org/vision/main/models/fcos.html } [Accessed: 13/03/2024]},
}

@Online{PyTorchd,
  author = {PyTorch},
  title  = {RetinaNet},
  url    = {{https://pytorch.org/vision/main/models/retinanet.html } [Accessed: 13/03/2024]},
}

@Online{Wanga,
  author = {Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  title  = {InternImage},
  url    = {{https://github.com/opengvlab/internimage} [Accessed: 13/03/2024]},
}

@Online{PapersWithCodea,
  author = {PapersWithCode},
  title  = {Torchvision},
  url    = {{https://paperswithcode.com/lib/torchvision } [Accessed: 13/03/2024]},
}

@Online{PapersWithCodeb,
  author = {PapersWithCode},
  title  = {NAS-FCOS: Fast Neural Architecture Search for Object Detection},
  url    = {{https://paperswithcode.com/paper/nas-fcos-fast-neural-architecture-search-for} [Accessed: 13/03/2024]},
}

@Article{DBLP:journals/corr/abs-1906-11172,
  author     = {Barret Zoph and Ekin D. Cubuk and Golnaz Ghiasi and Tsung{-}Yi Lin and Jonathon Shlens and Quoc V. Le},
  journal    = {CoRR},
  title      = {Learning Data Augmentation Strategies for Object Detection},
  year       = {2019},
  volume     = {abs/1906.11172},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1906-11172.bib},
  eprint     = {1906.11172},
  eprinttype = {arXiv},
  timestamp  = {Thu, 27 Jun 2019 18:54:51 +0200},
  url        = {http://arxiv.org/abs/1906.11172},
}

@Online{Nelson2020,
  author = {Joseph Nelson},
  day    = {26},
  month  = {01},
  title  = {What is Image Preprocessing and Augmentation?},
  url    = {{https://blog.roboflow.com/why-preprocess-augment/} [Accessed: 20/03/2024]},
  year   = {2020},
}

@InProceedings{Chen_2020,
  author     = {Chen, Jieshan and Xie, Mulong and Xing, Zhenchang and Chen, Chunyang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang},
  booktitle  = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  title      = {Object detection for graphical user interface: old fashioned or deep learning or a combination?},
  year       = {2020},
  month      = nov,
  publisher  = {ACM},
  series     = {ESEC/FSE ’20},
  collection = {ESEC/FSE ’20},
  doi        = {10.1145/3368089.3409691},
  url        = {http://dx.doi.org/10.1145/3368089.3409691},
}

@InProceedings{Kumar_2022,
  author    = {Kumar, Anurendra and Morabia, Keval and Wang, William and Chang, Kevin and Schwing, Alex},
  booktitle = {Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5)},
  title     = {CoVA: Context-aware Visual Attention for Webpage Information Extraction},
  year      = {2022},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.ecnlp-1.11},
  url       = {http://dx.doi.org/10.18653/v1/2022.ecnlp-1.11},
}

@InProceedings{10.1007/978-3-319-44944-9_14,
  author    = {Gogar, Tomas and Hubacek, Ondrej and Sedivy, Jan},
  booktitle = {Artificial Intelligence Applications and Innovations},
  title     = {Deep Neural Networks for Web Page Information Extraction},
  year      = {2016},
  address   = {Cham},
  editor    = {Iliadis, Lazaros and Maglogiannis, Ilias},
  pages     = {154--163},
  publisher = {Springer International Publishing},
  abstract  = {Web wrappers are systems for extracting structured information from web pages. Currently, wrappers need to be adapted to a particular website template before they can start the extraction process. In this work we present a new method, which uses convolutional neural networks to learn a wrapper that can extract information from previously unseen templates. Therefore, this wrapper does not need any site-specific initialization and is able to extract information from a single web page. We also propose a method for spatial text encoding, which allows us to encode visual and textual content of a web page into a single neural net. The first experiments with product information extraction showed very promising results and suggest that this approach can lead to a general site-independent web wrapper.},
  isbn      = {978-3-319-44944-9},
}

@InBook{inbook,
  author = {Altinbas, Mehmet and Serif, Tacha},
  pages  = {32-45},
  title  = {GUI Element Detection from Mobile UI Images Using YOLOv5},
  year   = {2022},
  isbn   = {978-3-031-14390-8},
  month  = {08},
  doi    = {10.1007/978-3-031-14391-5_3},
}

@InProceedings{10.1145/1622176.1622213,
  author    = {Yeh, Tom and Chang, Tsung-Hsiang and Miller, Robert C.},
  booktitle = {Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology},
  title     = {Sikuli: using GUI screenshots for search and automation},
  year      = {2009},
  address   = {New York, NY, USA},
  pages     = {183–192},
  publisher = {Association for Computing Machinery},
  series    = {UIST '09},
  abstract  = {We present Sikuli, a visual approach to search and automation of graphical user interfaces using screenshots. Sikuli allows users to take a screenshot of a GUI element (such as a toolbar button, icon, or dialog box) and query a help system using the screenshot instead of the element's name. Sikuli also provides a visual scripting API for automating GUI interactions, using screenshot patterns to direct mouse and keyboard events. We report a web-based user study showing that searching by screenshot is easy to learn and faster to specify than keywords. We also demonstrate several automation tasks suitable for visual scripting, such as map navigation and bus tracking, and show how visual scripting can improve interactive help systems previously proposed in the literature.},
  doi       = {10.1145/1622176.1622213},
  isbn      = {9781605587455},
  keywords  = {online help, image search, automation},
  location  = {Victoria, BC, Canada},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1622176.1622213},
}

@Online{Xie,
  author = {Mulong Xie},
  title  = {UIED},
  url    = {{https://github.com/MulongXie/UIED?tab=readme-ov-file} [Accessed: 27/03/2024]},
}

@Comment{jabref-meta: databaseType:bibtex;}
